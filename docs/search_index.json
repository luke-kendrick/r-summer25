[["index.html", "R Training Summer 2025 Psychology @ Royal Holloway University of London R Training", " R Training Summer 2025 Luke Kendrick / Victoria Bourne 2025-07-21 Psychology @ Royal Holloway University of London R Training Summer/Autumn Sessions 2025 "],["session-1-starting-with-r.html", "1 Session 1: Starting with R 1.1 Download Slides and Data Files 1.2 Code Used in this Session:", " 1 Session 1: Starting with R 1.1 Download Slides and Data Files Install R and RStudio Here Slides: here. Data file: height.csv here. Data file sleep.csv here. Data file: intervention.csv here. 1.2 Code Used in this Session: This is the code used from slide 42: 1.2.1 Install and Load Package: install.packages(&quot;tidyverse&quot;) # install only if needed library(tidyverse) # always load the package before starting Code for the first data file: data &lt;- read_csv(&quot;height.csv&quot;) print(data) view(data) 1.2.2 Calculating Descriptive Statistics rm(data) # will remove the object called “data”. data &lt;- read_csv(“sleep.csv”) # new data set Explore the new data set: head(data) #view the first few rows summary(data) #quick summary of the data set names(data) #check variable names Count and pipe %&gt;%: data %&gt;% count(condition) Means and Standard Deviations: descr &lt;- data %&gt;% summarise(mean_age = mean(age), sd_age = sd(age), mean_change = mean(change)) view(desc) # view the descriptives Add standard deviation for change: descr &lt;- data %&gt;% summarise(mean_age = mean(age), sd_age = sd(age), mean_change = mean(change) sd_change = sd(change)) view(desc) # view the updated descriptives Using group_by(): descr &lt;- data %&gt;% group_by(condition) %&gt;% summarise(mean_age = mean(age), sd_age = sd(age), mean_change = mean(change) sd_change = sd(change)) 1.2.3 Distributions Histogram: ggplot(data, aes(x = change, fill = condition)) + geom_histogram(colour = &quot;black&quot;) facet_wrap() ggplot(data, aes(x = change, fill = condition)) + geom_histogram(colour = &quot;black&quot;) + facet_wrap(~ condition) Density Plot: ggplot(data, aes(x = change, fill = condition)) + geom_density(alpha = .5) facet_wrap() ggplot(data, aes(x = change, fill = condition)) + geom_density(alpha = .5) + facet_wrap(~ condition) Box Plot: ggplot(data, aes(x = condition, y = change)) + geom_boxplot(width = .4) + theme_classic() 1.2.4 Wide-form to Long-form Data: Wide_data &lt;- read_csv(“intervention.csv”) view(wide_data) names(wide_data) long_data &lt;- wide_data %&gt;% pivot_longer(cols = c(pre, post), names_to = &quot;time_point&quot;, values_to = &quot;sleep_score&quot;) "],["session-2-correlation-and-regression.html", "2 Session 2: Correlation and Regression 2.1 Code Used in this Session:", " 2 Session 2: Correlation and Regression Slides: here. Lecture data file here Workshop data file: here. 2.1 Code Used in this Session: 2.1.1 SETTING UP # Before doing any analyses, you need to get everything set up and ready... # Set the working directory -WD- so R knows where the data lives. Do this by going Session &gt; Set working directory &gt; Choose directory # You can check the working directory... getwd() #Before doing anything, need to make sure the right packages are installed and open. We will use all of these...... install.packages(tidyverse) install.packages(correlation) install.packages(gridExtra) install.packages(ppcor) install.packages(cocor) install.packages(car) library(tidyverse) library(correlation) library(gridExtra) library(ppcor) library(cocor) library(car) Now get R to open our dataset: mydata &lt;- read_csv(&quot;Lecture_data_R_sat_life.csv&quot;) Check data: # To check the data have opened ok, you can view the data... view(mydata) # You can also check the number of participants (obs) and the number of variables in the &quot;Environment&quot; tab. # Next, we need to tell R which variables are continuous (as.numeric) and which are categorical (as.factor). # You can check the names of the variables with this... names(mydata) If necessary: mydata$p_num &lt;- as.numeric(mydata$p_num) mydata$sat_life &lt;- as.numeric(mydata$sat_life) mydata$psych_wellbeing &lt;- as.numeric(mydata$psych_wellbeing) mydata$physical_wellbeing &lt;- as.numeric(mydata$physical_wellbeing) mydata$relationship_wellbeing &lt;- as.numeric(mydata$relationship_wellbeing) mydata$neg_life_experiences &lt;- as.numeric(mydata$neg_life_experiences) mydata$occ_status &lt;- as.factor(mydata$occ_status) mydata$relationship_status &lt;- as.factor(mydata$relationship_status) mydata$home_location &lt;- as.factor(mydata$home_location) mydata$years_edu &lt;- as.numeric(mydata$years_edu) 2.1.2 RUNNING THE DESCRIPTIVE STATISTICS # Before getting into correlations, we might want a summary of our variables. For the continuous variables, we get descriptives. For the categorical/binary variables, we get frequencies. summary(mydata) # If we want to see the descriptives split for different groups, for example, we want to see the descriptives for satisfaction with life for occupational status separately.. descriptives_bygroup &lt;- mydata %&gt;% # Tell R which data set to use. %&gt;% means &quot;and then&quot; so tells R to move on and do something else group_by(occ_status) %&gt;% # group_by is telling R to split the data file - put the variable to split by in brackets summarise(mean_sat_life = mean(sat_life), sd_sat_life = sd(sat_life)) # Ask for the mean and standard deviation. statistic_calculated = statistic(variable ) # You then need R to &quot;print&quot; - or display - the calculated descriptives in the console window. print(descriptives_bygroup) 2.1.3 CREATING SCATTERPLOTS # First, let&#39;s graph the correlations between &quot;satisfaction&quot; with life, and the four other continuous variables. You won&#39;t see them until after you make them and then ask R to display them. We will make four scatterplots... # Satisfaction with life and psychological wellbeing plot1 &lt;- ggplot(mydata, aes(x = psych_wellbeing, y = sat_life)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_classic() # Satisfaction with life and physical wellbeing plot2 &lt;- ggplot(mydata, aes(x = physical_wellbeing, y = sat_life)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_classic() # Satisfaction with life and relationship wellbeing plot3 &lt;- ggplot(mydata, aes(x = relationship_wellbeing, y = sat_life)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_classic() # Satisfaction with life and negative life events plot4 &lt;- ggplot(mydata, aes(x = neg_life_experiences, y = sat_life)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_classic() # To see what the plots look like, we need to arrange them in the &quot;Plot&quot; window. Make sure &quot;gridextra&quot; is ticked in the &quot;Packages&quot; window grid.arrange(plot1, plot2, plot3, plot4, nrow = 2, ncol = 2) # ncol = 2 tells R to put two next to each other, nrow = 2 tells R to put one above the other # To just print one of the plots... print(plot1) 2.1.4 RUNNING PEARSON’S CORRELATIONS # Now we can look at the correlations between these four continuous variables - but we want to mainly focus on the correlations with satisfaction with life - our main variable of interest. mydata %&gt;% dplyr::select(sat_life, psych_wellbeing, physical_wellbeing, relationship_wellbeing, neg_life_experiences) %&gt;% correlation(p_adjust = &quot;none&quot;) # In addition to giving you the r and p values, it gives the N, so check that this is correct. To write up the correlation, remember that df = N-2. 2.1.5 RUNNING PARTIAL CORRELATIONS # Next, let&#39;s look at partial correlations, so the four main correlations of interest we just ran, but now controlling for years of education. # You need to have one set of code for each partial correlation, and make sure &quot;ppcor&quot; is ticked in the &quot;Packages&quot; window. pcor.test(mydata$sat_life, mydata$psych_wellbeing, mydata$years_edu, method = &quot;pearson&quot;) pcor.test(mydata$sat_life, mydata$physical_wellbeing, mydata$years_edu, method = &quot;pearson&quot;) pcor.test(mydata$sat_life, mydata$relationship_wellbeing, mydata$years_edu, method = &quot;pearson&quot;) pcor.test(mydata$sat_life, mydata$neg_life_experiences, mydata$years_edu, method = &quot;pearson&quot;) 2.1.6 COMPARING TWO CORRELATIONS # Final thing is comparing correlations across different groups. For example, is the correlation between satisfaction with life and negative life experiences different when comparing people who are single or in a relationship? # First, we need to tell R which subgroups within our dataset we want to look at - so identify 0 and 1 from the &quot;relationship&quot; variable, and name each one. single &lt;- mydata[mydata$relationship_status == &quot;0&quot;, ] relationship &lt;- mydata[mydata$relationship_status == &quot;1&quot;, ] # Now we run the two correlations - we need to tell R first which subgroup to use from the naming we just did, and which continuous variable to correlate. cor.test(single$sat_life, single$neg_life_experiences, method = &quot;pearson&quot;) cor.test(relationship$sat_life, relationship$neg_life_experiences, method = &quot;pearson&quot;) # To compare the correlations statistically, we need the N and the r for each group. The r value is the final value in the output we just created - the final line, under corr. # To get the N for each group, we ran the &quot;summary&quot; earlier, but you can do it again to save scrolling. summary(mydata) # Now we can statistically compare our r values. Make a note of which group you consider to be &quot;1&quot; and which is &quot;2&quot;. For this, it will be 1 is single and 2 is in a relationship. # First, make sure &quot;cocor&quot; is ticked in the &quot;Packages&quot; tab. # r1 is the first r-value, in this case -0.5218863 # r2 is the second r-value, in this case -0.1741089 # n1 is the first sample size, in this case 92 # n2 is the second sample size, in this case 108 # the code looks like this, so just replace the values as needed... cocor.indep.groups(r1, r2, n1, n2) cocor.indep.groups(-0.5218863, -0.1741089, 92, 108) # Final thing to do - graph these two correlations on the same plot to aid interpretation. plot_cc &lt;- ggplot(mydata, aes(x = neg_life_experiences, y = sat_life, colour = relationship_status)) + geom_point(aes(shape = relationship_status)) + geom_smooth(aes(linetype = relationship_status), method = &quot;lm&quot;, se = FALSE) + labs(title = &quot;Negative life experiences vs Satisfaction with life by Relationship status&quot;, x = &quot;Negative life experiences&quot;, y = &quot;Satisfaction with life&quot;) + theme_classic() + scale_color_manual(values = c(&quot;0&quot; = &quot;grey&quot;, &quot;1&quot; = &quot;black &quot;)) + scale_linetype_manual(values = c(&quot;0&quot; = &quot;solid&quot;, &quot;1&quot; = &quot;dashed&quot;)) + scale_shape_manual(values = c(&quot;0&quot; = 16, &quot;1&quot; = 3)) print(plot_cc) 2.1.7 MULTIPLE REGRESSION WITH CONTINUOUS PREDICTOR VARIABLES # Now, let&#39;s run a multiple regression. # We have SWL as the outcome variable that we want to predict # Then the three wellbeing measures and the number of negative life experiences giving us four continuous predictor variable. model &lt;- lm(sat_life ~ psych_wellbeing + physical_wellbeing + relationship_wellbeing + neg_life_experiences, data = mydata) summary(model) # We then want to create scatterplots to graphically represent any significant predictors (so three) # Satisfaction with life and psychological wellbeing plot1 &lt;- ggplot(mydata, aes(x = psych_wellbeing, y = sat_life)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_classic() # Satisfaction with life and relationship wellbeing plot2 &lt;- ggplot(mydata, aes(x = relationship_wellbeing, y = sat_life)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_classic() # Satisfaction with life and negative life events plot3 &lt;- ggplot(mydata, aes(x = neg_life_experiences, y = sat_life)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_classic() # To see what the plots look like, we need to arrange them in the &quot;Plot&quot; window. Make sure &quot;gridextra&quot; is ticked in the &quot;Packages&quot; window grid.arrange(plot1, plot2, plot3, nrow = 1, ncol = 3) # ncol = 3 tells R to put two next to each other, nrow = 1 tells R to put one above the other 2.1.8 HIERARCHICAL REGRESSION # Now, let&#39;s move on to hierarchical regression - exactly what we just did, but adding years of education as a control variable. # First, let&#39;s see if the control variable is significant by building model 1. Make sure it is called &quot;model 1&quot; and you need to run the summary to see the output. model1 &lt;- lm(sat_life ~ years_edu, data = mydata) summary(model1) # Next, build our final model that has all the variables (control and predictor variables). This is &quot;model 2&quot;, and again, use the summary to see the output. model2 &lt;- lm(sat_life ~ years_edu + psych_wellbeing + physical_wellbeing + relationship_wellbeing + neg_life_experiences, data = mydata) summary(model2) # Finally, we want to see if adding the predictor variables is significantly &quot;better&quot; than the control variable alone, this has two parts. # First - how much does the variance explained (adjusted R sq) increase? r2_control &lt;- summary(model1)$adj.r.squared # Adj Rsq of control model r2_full &lt;- summary(model2)$adj.r.squared # Adj Rsq of full model r2_change &lt;- r2_full - r2_control print(r2_change) # Print the Adj Rsq change # Does the model significantly improve? anova(model1,model2) # We then want to create scatterplots to graphically represent any significant predictors (so two) # Satisfaction with life and psychological wellbeing plot1 &lt;- ggplot(mydata, aes(x = psych_wellbeing, y = sat_life)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_classic() # Satisfaction with life and negative life events plot2 &lt;- ggplot(mydata, aes(x = neg_life_experiences, y = sat_life)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_classic() # To see what the plots look like, we need to arrange them in the &quot;Plot&quot; window. Make sure &quot;gridextra&quot; is ticked in the &quot;Packages&quot; window grid.arrange(plot1, plot2, nrow = 1, ncol = 2) 2.1.9 MULTIPLE REGRESSION WITH CONTINUOUS AND BINARY PREDICTORS # Now, let&#39;s run a multiple regression, but this time adding in the three new binary predictors # We have SWL as the outcome variable that we want to predict # Then four continuous predictors (the three wellbeing measures and the number of negative life experiences) and three binary (occupational status, relationship status, home location). model &lt;- lm(sat_life ~ psych_wellbeing + physical_wellbeing + relationship_wellbeing + neg_life_experiences + occ_status + relationship_status + home_location, data = mydata) summary(model) # We then want to create scatterplots to show the significant continuous predictors and boxplots to show the significant binary predictors. # First, build the two scatterplots (using the code from previous weeks) # Satisfaction with life and psychological wellbeing plot1 &lt;- ggplot(mydata, aes(x = psych_wellbeing, y = sat_life)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_classic() # Satisfaction with life and negative life events plot2 &lt;- ggplot(mydata, aes(x = neg_life_experiences, y = sat_life)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_classic() # To see what the scatterplots look like, we need to arrange them in the &quot;Plot&quot; window. Make sure &quot;gridextra&quot; is ticked in the &quot;Packages&quot; window grid.arrange(plot1, plot2, nrow = 1, ncol = 2) # ncol = 2 tells R to put two next to each other, nrow = 1 tells R to have one &quot;row&quot; of graphs, so puts them next to each other. # Next, we want to create a boxplot for each significant binary predictor. We do this in exactly the same way as for graphing an independent t test, so you can go back to that lecture/workshop if needed. ggplot(mydata, aes(x = occ_status, y = sat_life)) + geom_boxplot() + labs(title = &quot;Satisfaction with life by occupational status&quot;, x = &quot;Occupational status&quot;, y = &quot;Mean satisfaction with life score&quot;) + theme_classic() ggplot(mydata, aes(x = relationship_status, y = sat_life)) + geom_boxplot() + labs(title = &quot;Satisfaction with life by relationship status&quot;, x = &quot;Relationship status&quot;, y = &quot;Mean satisfaction with life score&quot;) + theme_classic() # To interpret the binary predictors, you might also want to look at the descriptive variables for each group separately. # First, looking by occupational status. descriptives_bygroup &lt;- mydata %&gt;% # Tell R which data set to use. %&gt;% means &quot;and then&quot; so tells R to move on and do something else group_by(occ_status) %&gt;% # group_by is telling R to split the data file - put the variable to split by in brackets summarise(mean_sat_life = mean(sat_life), sd_sat_life = sd(sat_life)) # Ask for the mean and standard deviation. statistic_calculated = statistic(variable ) # You then need R to &quot;print&quot; - or display - the calculated descriptives in the console window. print(descriptives_bygroup) # Next,repeat this, but looking by home location. descriptives_bygroup &lt;- mydata %&gt;% # Tell R which data set to use. %&gt;% means &quot;and then&quot; so tells R to move on and do something else group_by(home_location) %&gt;% # group_by is telling R to split the data file - put the variable to split by in brackets summarise(mean_sat_life = mean(sat_life), sd_sat_life = sd(sat_life)) # Ask for the mean and standard deviation. statistic_calculated = statistic(variable ) # You then need R to &quot;print&quot; - or display - the calculated descriptives in the console window. print(descriptives_bygroup) 2.1.10 MULTIPLE REGRESSION WITH INTERACTIVE PREDICTORS # Now, let&#39;s move on to looking at a very simple interactive predictor. For ease of teaching, we will simplify the model... # Predictor 1: negative life experiences (continuous predictor) # Predictor 2: relationship status (binary predictor) # Predictor 3: NLE by relationship status (interactive predictor) model &lt;- lm(sat_life ~ neg_life_experiences + relationship_status + neg_life_experiences*relationship_status, data = mydata) summary(model) How do you break down and understand a significant interactive predictor? Remember comparing correlations - go back to that! # Go back to the code we used to statistically compare correlations, and adapt it (if/where needed) to map onto NLE * relationship status predicting SWL... # First, we need to tell R which subgroups within our dataset we want to look at - so identify 0 and 1 from the &quot;relationship_status&quot; variable, and name each one. single &lt;- mydata[mydata$relationship_status == &quot;0&quot;, ] relationship &lt;- mydata[mydata$relationship_status == &quot;1&quot;, ] # Now we run the two correlations - we need to tell R first which subgroup to use from the naming we just did, and which continuous variable to correlate. cor.test(single$sat_life, single$neg_life_experiences, method = &quot;pearson&quot;) cor.test(relationship$sat_life, relationship$neg_life_experiences, method = &quot;pearson&quot;) # To compare the correlations statistically, we need the N and the r for each group. The r value is the final value in the output we just created - the final line, under corr. # To get the N for each group, we ran the &quot;summary&quot; earlier, but you can do it again to save scrolling. summary(mydata) # Now we can statistically compare our r values. Make a note of which group you consider to be &quot;1&quot; and which is &quot;2&quot;. For this, it will be 1 is single and 2 is in a relationship. # First, make sure &quot;cocor&quot; is ticked in the &quot;Packages&quot; tab. # r1 is the first r-value, in this case -0.5218863 # r2 is the second r-value, in this case -0.1741089 # n1 is the first sample size, in this case 92 # n2 is the second sample size, in this case 108 # the code looks like this, so just replace the values as needed... cocor.indep.groups(r1, r2, n1, n2) cocor.indep.groups(-0.5218863, -0.1741089, 92, 108) # Final thing to do - graph these two correlations on the same plot to aid interpretation. plot_cc &lt;- ggplot(mydata, aes(x = neg_life_experiences, y = sat_life, colour = relationship_status)) + geom_point(aes(shape = relationship_status)) + geom_smooth(aes(linetype = relationship_status), method = &quot;lm&quot;, se = FALSE) + labs(title = &quot;Negative life experiences vs Satisfaction with life by Relationship status&quot;, x = &quot;Negative life experiences&quot;, y = &quot;Satisfaction with life&quot;) + theme_classic() + scale_color_manual(values = c(&quot;0&quot; = &quot;grey&quot;, &quot;1&quot; = &quot;black&quot;)) + scale_linetype_manual(values = c(&quot;0&quot; = &quot;solid&quot;, &quot;1&quot; = &quot;dashed&quot;)) + scale_shape_manual(values = c(&quot;0&quot; = 16, &quot;1&quot; = 3)) #Now show the graph in the &quot;Plots&quot; window... print(plot_cc) 2.1.11 ASSUMPTIONS OF MULTIPLE REGRESSION # Finally, time to look at assumptions! # First, the the regression - you report the regression after the assumptions, but R will need the &quot;model&quot; for the assumptions code. model &lt;- lm(sat_life ~ psych_wellbeing + physical_wellbeing + relationship_wellbeing + neg_life_experiences + occ_status + relationship_status + home_location, data = mydata) summary(model) 2.1.11.1 MULTICOLLINEARITY # Multicollinearity, looking at r values across all continuous predictor variables. mydata %&gt;% dplyr::select(psych_wellbeing, physical_wellbeing, relationship_wellbeing, neg_life_experiences) %&gt;% correlation(p_adjust = &quot;none&quot;) # Multicollinearity, calculate VIF. vif_values &lt;- vif(model) print(vif_values) # Multicollinearity, calculate tolerance. This is, essentially 1 - the R2 (so variance explained). tolerance_value &lt;- 1 - summary(model)$r.squared print(tolerance_value) 2.1.11.2 DISTRIBUTION OF RESIDUALS # Create the histogram of residuals ggplot(mydata, aes(x = model$residuals)) + geom_histogram(binwidth = 0.5, color = &quot;black&quot;, fill = &quot;white&quot;) + labs(title = &quot;Histogram of Residuals&quot;, x = &quot;Residuals&quot;, y = &quot;Frequency&quot;) + theme_minimal() 2.1.11.3 HOMOSCEDASTICITY # Create the scatterplot for homoscedasticity ggplot(mydata, aes(x = model$fitted.values, y = model$residuals)) + geom_point() + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;) + labs(title = &quot;Residuals vs Fitted Values&quot;, x = &quot;Fitted Values&quot;, y = &quot;Residuals&quot;) + theme_minimal() 2.1.11.4 EVALUATE THE NUMBER OF OUTLIERS # Identify outliers using standardized residuals standardized_residuals &lt;- rstandard(model) # Print standardized residuals print(standardized_residuals) # Determine the number of outliers (absolute value greater than 2) outliers &lt;- sum(abs(standardized_residuals) &gt; 2) print(outliers) # Calculate the percentage of outliers percentage_outliers &lt;- (outliers / nrow(mydata)) * 100 print(percentage_outliers) Lecture Data Script: here. Workshop Data Script: Slides: here. "],["session-3-t-tests-and-anovas.html", "3 Session 3: t-Tests and ANOVAs 3.1 Code Used in this Session 3.2 PS2010 Workshop: T-tests (Independent) 3.3 PS2010 Workshop: T-tests (Repeated) 3.4 Workshop 4: One-Way ANOVA (Independent) 3.5 Workshop 5: One-Way ANOVA (Repeated) 3.6 Factorial ANOVA", " 3 Session 3: t-Tests and ANOVAs 3.1 Code Used in this Session 3.2 PS2010 Workshop: T-tests (Independent) 3.2.1 Exercise One We need to call any packages! install.packages(&quot;rstatix&quot;) library(tidyverse) library(rstatix) Import Data mydata &lt;- read_csv(&quot;https://raw.githubusercontent.com/luke-kendrick/r-summer25/main/downloads/redcow2.csv&quot;) 3.2.2 Exercise Two Prepare/Check Data names(mydata) summary(mydata) 3.2.3 Exercise Three Describe Data # we want to produce some basic descriptive statistics for our data set. # we know that we want to compare redcow with water, so use the same code from last week to produce mean values. desc &lt;- mydata %&gt;% group_by(drink) %&gt;% summarise(mean = mean(sleep), sd = sd(sleep)) # now we will print our descriptive stats in something called a tibble print(desc) # print to console view(desc) # open in new tab, it is best to use this 3.2.4 Exercise Four Run the t-Test Run an independent t-test comparing sleep score for the redcow vs. water groups. # we want to run a t-test to compare the sleep score (dependent variable) across the two drink groups (independent variable: redcow and water) redcow_t &lt;- t.test(sleep ~ drink, mydata, var.equal = FALSE) print(redcow_t) # we also want to find and report the effect size. for a t-test we can use Cohen&#39;s d. mydata %&gt;% cohens_d(sleep ~ drink, var.equal = FALSE) 3.2.5 Exercise Five Check Assumptions # we want to check any necessary assumptions that might change how we interpret our model # we do not need to look at homogeneity of variance when using Welch&#39;s t-test. # we only need to look at normality. hist(mydata$sleep[mydata$drink == &quot;redcow&quot;]) #creates histogram for redcow hist(mydata$sleep[mydata$drink == &quot;water&quot;]) #creates histogram for water shapiro.test(mydata$sleep[mydata$drink == &quot;redcow&quot;]) #runs Shapiro test for redcow shapiro.test(mydata$sleep[mydata$drink == &quot;water&quot;]) #runs Shapiro test for water 3.2.6 Exercise Six (Optional) You will practice graphing your findings from next week. But for those of you who want a head start, feel free to run the code below to visually present your descriptive statistics. Copy and paste the code below. # use the code to create a box plot of your descriptive statistics. ggplot(mydata, aes(x = drink, y = sleep)) + geom_boxplot() + labs(title = &quot;Sleep Quality Score for RedCow vs Water&quot;, x = &quot;Drink&quot;, y = &quot;Mean Sleep Quality Score&quot;) + theme_classic() 3.3 PS2010 Workshop: T-tests (Repeated) Import Data mydata &lt;- read_csv(&quot;https://raw.githubusercontent.com/luke-kendrick/r-summer25/main/downloads/redcow3.csv&quot;) 3.3.1 Exercise One We need to call any packages! install.packages(&quot;rstatix&quot;) install.packages(&quot;effectsize&quot;) library(tidyverse) library(rstatix) library(effectsize) 3.3.2 Exercise Two Prepare/Check Data head(mydata) summary(mydata) It looks as though the data are not in long format. We need to create a new data set with the data in long format. Thankfully we can do this with some simple code: mydata_long &lt;- mydata %&gt;% # creates a new object called mydata_long pivot_longer(cols = c(before, after), # use pivot_longer() and select the column names. names_to = &quot;time&quot;, # give a column name for our independent variable values_to = &quot;bpm&quot;) # give a column name for our dependent variable Check that mydata_long has appeared in the environment (top right panel) 3.3.3 Exercise Three Descriptive Statistics Adapt the code below to produce descriptive statistics. You need to use the variable names from your long data file: time and bpm. Change NULL to the relevant details. desc &lt;- mydata_long %&gt;% # Which data set will you use? group_by(time) %&gt;% # what should you split the file by? summarise(mean = mean(bpm), sd = sd(bpm)) # mean and standard deviation of the dependent variable view(desc) 3.3.4 Exercise Four Run a repeated t-test comparing BPM before and after drinking RedCow. # we want to run a t-test to compare the bpm (dependent variable) across time (levels: before vs. after) before &lt;- mydata_long$bpm[mydata_long$time == &quot;before&quot;] # tell R which data is &quot;before&quot; after &lt;- mydata_long$bpm[mydata_long$time == &quot;after&quot;] # tell R which data is &quot;after&quot; redcow_t &lt;- t.test(before, after, paired = TRUE) print(redcow_t) # we also want to find and report the effect size. for a t-test we can use Cohen&#39;s d. cohens_d(before, after, paired = TRUE) #or try: t.test(bpm ~ time, mydata_long, paired = TRUE) 3.3.5 Exercise Five Check Assumptions Use the lines of code below to check the assumption of normality. Firstly, we need to calculate a difference score. # we need to check normality, but for the difference scores # first we need to calculate the difference score # we will go back to the original data file &quot;mydata&quot; for this, not &quot;mydata_long&quot; diff = mydata$before - mydata$after hist(diff) #creates histogram for the diff_score shapiro.test(diff) #runs Shapiro test for diff_score 3.4 Workshop 4: One-Way ANOVA (Independent) 3.4.1 Exercise One We need to call any packages! # only install packages if you don&#39;t have them installed already install.packages(&quot;emmeans&quot;) install.packages(&quot;afex&quot;) install.packages(&quot;car&quot;) # otherwise just call the packages library(tidyverse) library(afex) library(rstatix) library(emmeans) library(broom) library(car) Import Data mydata &lt;- read_csv(&quot;https://raw.githubusercontent.com/luke-kendrick/r-summer25/main/downloads/earlybird.csv&quot;) 3.4.2 Exercise Two Prepare/Check Data # have a quick check of your data file to learn about it. head(mydata) names(mydata) summary(mydata) 3.4.3 Exercise Three Descriptive Statistics Adapt the code below to produce descriptive statistics (replace NULL). desc &lt;- mydata %&gt;% # Which data set will you use? group_by(group) %&gt;% # what should you split the file by? summarise(mean = mean(alert), sd = sd(alert)) # mean and standard deviation of the dependent variable view(desc) We can also look at a box plot of our data. Use the code below to generate a box plot: #how about a box plot too ggplot(mydata, aes(x = group, y = alert, fill = group)) + geom_boxplot() + theme_classic() 3.4.4 Exercise Four RUn the ANOVA Run a one-way independent ANOVA comparing the three groups (lark, neither, owl) as the independent variable and the alertness score as dependent variable. For this exercise you should run a post-hoc test. #run the ANOVA model anova_result &lt;- aov_ez(id = &quot;id&quot;, # identifier variable dv = &quot;alert&quot;, # dependent variable variable name between = &quot;group&quot;, # between subjects variable name type = 3, # leave this as `3` include_aov = TRUE, # leave this as `TRUE` data = mydata) # name of your data file # just some code to help tidy the output anova_result_output &lt;- (anova_result$anova_table) %&gt;% tidy() view(anova_result_output) # we need eta squared for the effect size install.packages(&quot;effectsize&quot;) library(effectsize) eta_squared(anova_result, partial = FALSE) # asks for eta square Post-hoc Options First we need to run emmeans() # we need emmeans to run post-hocs em_means &lt;- emmeans(anova_result, ~ group) # group is the between subjects variable in our data Then set-up the pairwise comparisons and decide which alpha level adjustment to use. Pick one from either: 1. Holm 2. Bonferroni 3. Tukey HSD pairwise_contrasts &lt;- contrast(em_means, method = &quot;pairwise&quot;) # set up pairwise comparisons print(pairwise_contrasts, adjust = &quot;holm&quot;) # applies holm adjustment print(pairwise_contrasts, adjust = &quot;bonferroni&quot;) # applies bonf adjustment print(pairwise_contrasts, adjust = &quot;tukey&quot;) # applies tukey adjustment 3.4.5 Exercise Six Check Assumptions Use the code below to evaluate both normality and homogeneity of variance. #testing normality, looking at normal distribution of residuals from the model. residuals &lt;- residuals(anova_result) #pulls residuals from the model # produce QQ-Plot qqPlot(residuals) #produces a qq-plot of residuals # produce histogram of residuals hist(residuals, breaks = 20, main = &quot;Histogram of Residuals&quot;, xlab = &quot;Residuals&quot;, col = &quot;lightblue&quot;) # Shapiro test on residuals shapiro.test(residuals) #Shapiro test for residuals # testing homogeneity of variance leveneTest(alert ~ group, mydata) 3.5 Workshop 5: One-Way ANOVA (Repeated) 3.5.1 Exercise One We need to call any packages! library(tidyverse) library(broom) library(afex) library(emmeans) library(car) Import Data mydata &lt;- read_csv(&quot;https://raw.githubusercontent.com/luke-kendrick/r-summer25/main/downloads/memory.csv&quot;) 3.5.2 Exercise Two Prepare/Check Data head(mydata) names(mydata) summary(mydata) The data set does not appear to be in long/tidy form. We need to change that using pivot_longer. We will create a new version of the data but in long format. #it looks like the data are not in long format. we need to fix that. longdata &lt;- mydata %&gt;% # creates a new object called mydata_long pivot_longer(cols = c(year1, year2, year3, year4), # the existing column names for the IV names_to = &quot;time&quot;, # the name of your independent variable values_to = &quot;score&quot;) # the name of you dependent variable 3.5.3 Exercise Three Descriptive Statistics Adapt the code to produce descriptive stats in the same way you have done previously. *Hint: Use longdata as your data set. desc &lt;- NULL %&gt;% # Which data set will you use? group_by(NULL) %&gt;% # what should you split the file by? summarise(mean = mean(NULL), sd = sd(NULL)) # mean and standard deviation of the dependent variable view(desc) 3.5.4 Exercise Four Run the ANOVAs Run a one-way repeated ANOVA comparing memory score across the four time points (year1, year2, year3, year4). Adapt the code below. Change NULL to the relevant variables/information anova_result &lt;- aov_ez(id = &quot;NULL&quot;, dv = &quot;NULL&quot;, within = &quot;NULL&quot;, type = 3, include_aov = TRUE, data = NULL) # just some code to help tidy the output anova_result_output &lt;- (anova_result$anova_table) %&gt;% tidy() view(anova_result_output) # we need eta squared for the effect size library(effectsize) eta_squared(anova_result, partial = FALSE) # asks for eta square We might also want to print() the ANOVA results. This is because we can see if a Greenhouse-Geisser correction has been applied. You can check this using two methods: 1. If the degrees of freedom in the ANOVA output are not whole numbers, and have two decimal places then the assumption of Sphericity was not met and the model has automatically adjusted to account for this. 2. You can use the code below to print() the model and it will tell you if any correction was applied. print(anova_result) # prints the ANOVA result to the console (bottom left panel) Once you have looked at the ANOVA output and interpreted, if there is a significant difference you should run planned or post-hoc comparisons. For today’s example, we want to use a planned contrast. First we need to pull out the emmeans(). Change NULL to the independent variable in the model: time. # for any contrasts or post-hocs we need emmeans em_means &lt;- emmeans(anova_result, ~ NULL) # NULL should be the independent variable. Now let’s run the code for a polynomial contrast. #let&#39;s run a polynomial contrast poly_contrasts &lt;- contrast(em_means, method = &quot;poly&quot;) #sets up an runs the polynomial contrast print(poly_contrasts) It might help to visualise this effect. Maybe a line chart could help… #let&#39;s visualise the data to help our interpretation ggplot(mydata_long, aes(x = time, y = score, fill = time)) + stat_summary(fun = mean, geom = &quot;line&quot;, color = &quot;black&quot;, size = 1, aes(group = 1)) + # line for mean theme_classic() + labs(title = &quot;Plot of Memory Score Across Time&quot;, # add a title x = &quot;Time Point&quot;, # X-axis label y = &quot;Memory Score&quot;) # Y-axis label 3.5.5 Exercise Six Check Assumptions Use the code below to evaluate both normality and homogeneity of variance. This will run it for the anova_result model from exercise four. You can just add a c into the code to also check for the ANCOVA model. #testing normality, looking at normal distribution of residuals from the model. residuals &lt;- residuals(anova_result) #pulls residuals from the model # produce QQ-Plot qqPlot(residuals) #produces a qq-plot of residuals # produce histogram of residuals hist(residuals, breaks = 20, main = &quot;Histogram of Residuals&quot;, xlab = &quot;Residuals&quot;, col = &quot;lightblue&quot;) # Shapiro test on residuals shapiro.test(residuals) #Shapiro test for residuals 3.6 Factorial ANOVA "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
