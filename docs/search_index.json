[["index.html", "R Training Summer 2025 Psychology @ Royal Holloway University of London R Training", " R Training Summer 2025 Luke Kendrick / Victoria Bourne 2025-07-17 Psychology @ Royal Holloway University of London R Training Summer/Autumn Sessions 2025 "],["session-1-starting-with-r.html", "1 Session 1: Starting with R 1.1 Download Slides and Data Files 1.2 Code Used in this Session:", " 1 Session 1: Starting with R 1.1 Download Slides and Data Files Install R and RStudio Here Slides: here. Data file: height.csv here. Data file sleep.csv here. Data file: intervention.csv here. 1.2 Code Used in this Session: This is the code used from slide 42: 1.2.1 Install and Load Package: install.packages(&quot;tidyverse&quot;) # install only if needed library(tidyverse) # always load the package before starting Code for the first data file: data &lt;- read_csv(&quot;height.csv&quot;) print(data) view(data) 1.2.2 Calculating Descriptive Statistics rm(data) # will remove the object called “data”. data &lt;- read_csv(“sleep.csv”) # new data set Explore the new data set: head(data) #view the first few rows summary(data) #quick summary of the data set names(data) #check variable names Count and pipe %&gt;%: data %&gt;% count(condition) Means and Standard Deviations: descr &lt;- data %&gt;% summarise(mean_age = mean(age), sd_age = sd(age), mean_change = mean(change)) view(desc) # view the descriptives Add standard deviation for change: descr &lt;- data %&gt;% summarise(mean_age = mean(age), sd_age = sd(age), mean_change = mean(change) sd_change = sd(change)) view(desc) # view the updated descriptives Using group_by(): descr &lt;- data %&gt;% group_by(condition) %&gt;% summarise(mean_age = mean(age), sd_age = sd(age), mean_change = mean(change) sd_change = sd(change)) 1.2.3 Distributions Histogram: ggplot(data, aes(x = change, fill = condition)) + geom_histogram(colour = &quot;black&quot;) facet_wrap() ggplot(data, aes(x = change, fill = condition)) + geom_histogram(colour = &quot;black&quot;) + facet_wrap(~ condition) Density Plot: ggplot(data, aes(x = change, fill = condition)) + geom_density(alpha = .5) facet_wrap() ggplot(data, aes(x = change, fill = condition)) + geom_density(alpha = .5) + facet_wrap(~ condition) Box Plot: ggplot(data, aes(x = condition, y = change)) + geom_boxplot(width = .4) + theme_classic() 1.2.4 Wide-form to Long-form Data: Wide_data &lt;- read_csv(“intervention.csv”) view(wide_data) names(wide_data) long_data &lt;- wide_data %&gt;% pivot_longer(cols = c(pre, post), names_to = &quot;time_point&quot;, values_to = &quot;sleep_score&quot;) "],["session-2-correlation-and-regression.html", "2 Session 2: Correlation and Regression 2.1 Code Used in this Session:", " 2 Session 2: Correlation and Regression Slides: here. Lecture data file here Workshop data file: here. 2.1 Code Used in this Session: 2.1.1 SETTING UP # Before doing any analyses, you need to get everything set up and ready... # Set the working directory -WD- so R knows where the data lives. Do this by going Session &gt; Set working directory &gt; Choose directory # You can check the working directory... getwd() #Before doing anything, need to make sure the right packages are installed and open. We will use all of these...... install.packages(tidyverse) install.packages(correlation) install.packages(gridExtra) install.packages(ppcor) install.packages(cocor) install.packages(car) library(tidyverse) library(correlation) library(gridExtra) library(ppcor) library(cocor) library(car) Now get R to open our dataset: mydata &lt;- read_csv(&quot;Lecture_data_R_sat_life.csv&quot;) Check data: # To check the data have opened ok, you can view the data... view(mydata) # You can also check the number of participants (obs) and the number of variables in the &quot;Environment&quot; tab. # Next, we need to tell R which variables are continuous (as.numeric) and which are categorical (as.factor). # You can check the names of the variables with this... names(mydata) If necessary: mydata$p_num &lt;- as.numeric(mydata$p_num) mydata$sat_life &lt;- as.numeric(mydata$sat_life) mydata$psych_wellbeing &lt;- as.numeric(mydata$psych_wellbeing) mydata$physical_wellbeing &lt;- as.numeric(mydata$physical_wellbeing) mydata$relationship_wellbeing &lt;- as.numeric(mydata$relationship_wellbeing) mydata$neg_life_experiences &lt;- as.numeric(mydata$neg_life_experiences) mydata$occ_status &lt;- as.factor(mydata$occ_status) mydata$relationship_status &lt;- as.factor(mydata$relationship_status) mydata$home_location &lt;- as.factor(mydata$home_location) mydata$years_edu &lt;- as.numeric(mydata$years_edu) 2.1.2 RUNNING THE DESCRIPTIVE STATISTICS # Before getting into correlations, we might want a summary of our variables. For the continuous variables, we get descriptives. For the categorical/binary variables, we get frequencies. summary(mydata) # If we want to see the descriptives split for different groups, for example, we want to see the descriptives for satisfaction with life for occupational status separately.. descriptives_bygroup &lt;- mydata %&gt;% # Tell R which data set to use. %&gt;% means &quot;and then&quot; so tells R to move on and do something else group_by(occ_status) %&gt;% # group_by is telling R to split the data file - put the variable to split by in brackets summarise(mean_sat_life = mean(sat_life), sd_sat_life = sd(sat_life)) # Ask for the mean and standard deviation. statistic_calculated = statistic(variable ) # You then need R to &quot;print&quot; - or display - the calculated descriptives in the console window. print(descriptives_bygroup) 2.1.3 CREATING SCATTERPLOTS # First, let&#39;s graph the correlations between &quot;satisfaction&quot; with life, and the four other continuous variables. You won&#39;t see them until after you make them and then ask R to display them. We will make four scatterplots... # Satisfaction with life and psychological wellbeing plot1 &lt;- ggplot(mydata, aes(x = psych_wellbeing, y = sat_life)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_classic() # Satisfaction with life and physical wellbeing plot2 &lt;- ggplot(mydata, aes(x = physical_wellbeing, y = sat_life)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_classic() # Satisfaction with life and relationship wellbeing plot3 &lt;- ggplot(mydata, aes(x = relationship_wellbeing, y = sat_life)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_classic() # Satisfaction with life and negative life events plot4 &lt;- ggplot(mydata, aes(x = neg_life_experiences, y = sat_life)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_classic() # To see what the plots look like, we need to arrange them in the &quot;Plot&quot; window. Make sure &quot;gridextra&quot; is ticked in the &quot;Packages&quot; window grid.arrange(plot1, plot2, plot3, plot4, nrow = 2, ncol = 2) # ncol = 2 tells R to put two next to each other, nrow = 2 tells R to put one above the other # To just print one of the plots... print(plot1) 2.1.4 RUNNING PEARSON’S CORRELATIONS # Now we can look at the correlations between these four continuous variables - but we want to mainly focus on the correlations with satisfaction with life - our main variable of interest. mydata %&gt;% dplyr::select(sat_life, psych_wellbeing, physical_wellbeing, relationship_wellbeing, neg_life_experiences) %&gt;% correlation(p_adjust = &quot;none&quot;) # In addition to giving you the r and p values, it gives the N, so check that this is correct. To write up the correlation, remember that df = N-2. 2.1.5 RUNNING PARTIAL CORRELATIONS # Next, let&#39;s look at partial correlations, so the four main correlations of interest we just ran, but now controlling for years of education. # You need to have one set of code for each partial correlation, and make sure &quot;ppcor&quot; is ticked in the &quot;Packages&quot; window. pcor.test(mydata$sat_life, mydata$psych_wellbeing, mydata$years_edu, method = &quot;pearson&quot;) pcor.test(mydata$sat_life, mydata$physical_wellbeing, mydata$years_edu, method = &quot;pearson&quot;) pcor.test(mydata$sat_life, mydata$relationship_wellbeing, mydata$years_edu, method = &quot;pearson&quot;) pcor.test(mydata$sat_life, mydata$neg_life_experiences, mydata$years_edu, method = &quot;pearson&quot;) 2.1.6 COMPARING TWO CORRELATIONS # Final thing is comparing correlations across different groups. For example, is the correlation between satisfaction with life and negative life experiences different when comparing people who are single or in a relationship? # First, we need to tell R which subgroups within our dataset we want to look at - so identify 0 and 1 from the &quot;relationship&quot; variable, and name each one. single &lt;- mydata[mydata$relationship_status == &quot;0&quot;, ] relationship &lt;- mydata[mydata$relationship_status == &quot;1&quot;, ] # Now we run the two correlations - we need to tell R first which subgroup to use from the naming we just did, and which continuous variable to correlate. cor.test(single$sat_life, single$neg_life_experiences, method = &quot;pearson&quot;) cor.test(relationship$sat_life, relationship$neg_life_experiences, method = &quot;pearson&quot;) # To compare the correlations statistically, we need the N and the r for each group. The r value is the final value in the output we just created - the final line, under corr. # To get the N for each group, we ran the &quot;summary&quot; earlier, but you can do it again to save scrolling. summary(mydata) # Now we can statistically compare our r values. Make a note of which group you consider to be &quot;1&quot; and which is &quot;2&quot;. For this, it will be 1 is single and 2 is in a relationship. # First, make sure &quot;cocor&quot; is ticked in the &quot;Packages&quot; tab. # r1 is the first r-value, in this case -0.5218863 # r2 is the second r-value, in this case -0.1741089 # n1 is the first sample size, in this case 92 # n2 is the second sample size, in this case 108 # the code looks like this, so just replace the values as needed... cocor.indep.groups(r1, r2, n1, n2) cocor.indep.groups(-0.5218863, -0.1741089, 92, 108) # Final thing to do - graph these two correlations on the same plot to aid interpretation. plot_cc &lt;- ggplot(mydata, aes(x = neg_life_experiences, y = sat_life, colour = relationship_status)) + geom_point(aes(shape = relationship_status)) + geom_smooth(aes(linetype = relationship_status), method = &quot;lm&quot;, se = FALSE) + labs(title = &quot;Negative life experiences vs Satisfaction with life by Relationship status&quot;, x = &quot;Negative life experiences&quot;, y = &quot;Satisfaction with life&quot;) + theme_classic() + scale_color_manual(values = c(&quot;0&quot; = &quot;grey&quot;, &quot;1&quot; = &quot;black &quot;)) + scale_linetype_manual(values = c(&quot;0&quot; = &quot;solid&quot;, &quot;1&quot; = &quot;dashed&quot;)) + scale_shape_manual(values = c(&quot;0&quot; = 16, &quot;1&quot; = 3)) print(plot_cc) 2.1.7 MULTIPLE REGRESSION WITH CONTINUOUS PREDICTOR VARIABLES # Now, let&#39;s run a multiple regression. # We have SWL as the outcome variable that we want to predict # Then the three wellbeing measures and the number of negative life experiences giving us four continuous predictor variable. model &lt;- lm(sat_life ~ psych_wellbeing + physical_wellbeing + relationship_wellbeing + neg_life_experiences, data = mydata) summary(model) # We then want to create scatterplots to graphically represent any significant predictors (so three) # Satisfaction with life and psychological wellbeing plot1 &lt;- ggplot(mydata, aes(x = psych_wellbeing, y = sat_life)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_classic() # Satisfaction with life and relationship wellbeing plot2 &lt;- ggplot(mydata, aes(x = relationship_wellbeing, y = sat_life)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_classic() # Satisfaction with life and negative life events plot3 &lt;- ggplot(mydata, aes(x = neg_life_experiences, y = sat_life)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_classic() # To see what the plots look like, we need to arrange them in the &quot;Plot&quot; window. Make sure &quot;gridextra&quot; is ticked in the &quot;Packages&quot; window grid.arrange(plot1, plot2, plot3, nrow = 1, ncol = 3) # ncol = 3 tells R to put two next to each other, nrow = 1 tells R to put one above the other 2.1.8 HIERARCHICAL REGRESSION # Now, let&#39;s move on to hierarchical regression - exactly what we just did, but adding years of education as a control variable. # First, let&#39;s see if the control variable is significant by building model 1. Make sure it is called &quot;model 1&quot; and you need to run the summary to see the output. model1 &lt;- lm(sat_life ~ years_edu, data = mydata) summary(model1) # Next, build our final model that has all the variables (control and predictor variables). This is &quot;model 2&quot;, and again, use the summary to see the output. model2 &lt;- lm(sat_life ~ years_edu + psych_wellbeing + physical_wellbeing + relationship_wellbeing + neg_life_experiences, data = mydata) summary(model2) # Finally, we want to see if adding the predictor variables is significantly &quot;better&quot; than the control variable alone, this has two parts. # First - how much does the variance explained (adjusted R sq) increase? r2_control &lt;- summary(model1)$adj.r.squared # Adj Rsq of control model r2_full &lt;- summary(model2)$adj.r.squared # Adj Rsq of full model r2_change &lt;- r2_full - r2_control print(r2_change) # Print the Adj Rsq change # Does the model significantly improve? anova(model1,model2) # We then want to create scatterplots to graphically represent any significant predictors (so two) # Satisfaction with life and psychological wellbeing plot1 &lt;- ggplot(mydata, aes(x = psych_wellbeing, y = sat_life)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_classic() # Satisfaction with life and negative life events plot2 &lt;- ggplot(mydata, aes(x = neg_life_experiences, y = sat_life)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_classic() # To see what the plots look like, we need to arrange them in the &quot;Plot&quot; window. Make sure &quot;gridextra&quot; is ticked in the &quot;Packages&quot; window grid.arrange(plot1, plot2, nrow = 1, ncol = 2) 2.1.9 MULTIPLE REGRESSION WITH CONTINUOUS AND BINARY PREDICTORS # Now, let&#39;s run a multiple regression, but this time adding in the three new binary predictors # We have SWL as the outcome variable that we want to predict # Then four continuous predictors (the three wellbeing measures and the number of negative life experiences) and three binary (occupational status, relationship status, home location). model &lt;- lm(sat_life ~ psych_wellbeing + physical_wellbeing + relationship_wellbeing + neg_life_experiences + occ_status + relationship_status + home_location, data = mydata) summary(model) # We then want to create scatterplots to show the significant continuous predictors and boxplots to show the significant binary predictors. # First, build the two scatterplots (using the code from previous weeks) # Satisfaction with life and psychological wellbeing plot1 &lt;- ggplot(mydata, aes(x = psych_wellbeing, y = sat_life)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_classic() # Satisfaction with life and negative life events plot2 &lt;- ggplot(mydata, aes(x = neg_life_experiences, y = sat_life)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_classic() # To see what the scatterplots look like, we need to arrange them in the &quot;Plot&quot; window. Make sure &quot;gridextra&quot; is ticked in the &quot;Packages&quot; window grid.arrange(plot1, plot2, nrow = 1, ncol = 2) # ncol = 2 tells R to put two next to each other, nrow = 1 tells R to have one &quot;row&quot; of graphs, so puts them next to each other. # Next, we want to create a boxplot for each significant binary predictor. We do this in exactly the same way as for graphing an independent t test, so you can go back to that lecture/workshop if needed. ggplot(mydata, aes(x = occ_status, y = sat_life)) + geom_boxplot() + labs(title = &quot;Satisfaction with life by occupational status&quot;, x = &quot;Occupational status&quot;, y = &quot;Mean satisfaction with life score&quot;) + theme_classic() ggplot(mydata, aes(x = relationship_status, y = sat_life)) + geom_boxplot() + labs(title = &quot;Satisfaction with life by relationship status&quot;, x = &quot;Relationship status&quot;, y = &quot;Mean satisfaction with life score&quot;) + theme_classic() # To interpret the binary predictors, you might also want to look at the descriptive variables for each group separately. # First, looking by occupational status. descriptives_bygroup &lt;- mydata %&gt;% # Tell R which data set to use. %&gt;% means &quot;and then&quot; so tells R to move on and do something else group_by(occ_status) %&gt;% # group_by is telling R to split the data file - put the variable to split by in brackets summarise(mean_sat_life = mean(sat_life), sd_sat_life = sd(sat_life)) # Ask for the mean and standard deviation. statistic_calculated = statistic(variable ) # You then need R to &quot;print&quot; - or display - the calculated descriptives in the console window. print(descriptives_bygroup) # Next,repeat this, but looking by home location. descriptives_bygroup &lt;- mydata %&gt;% # Tell R which data set to use. %&gt;% means &quot;and then&quot; so tells R to move on and do something else group_by(home_location) %&gt;% # group_by is telling R to split the data file - put the variable to split by in brackets summarise(mean_sat_life = mean(sat_life), sd_sat_life = sd(sat_life)) # Ask for the mean and standard deviation. statistic_calculated = statistic(variable ) # You then need R to &quot;print&quot; - or display - the calculated descriptives in the console window. print(descriptives_bygroup) 2.1.10 MULTIPLE REGRESSION WITH INTERACTIVE PREDICTORS # Now, let&#39;s move on to looking at a very simple interactive predictor. For ease of teaching, we will simplify the model... # Predictor 1: negative life experiences (continuous predictor) # Predictor 2: relationship status (binary predictor) # Predictor 3: NLE by relationship status (interactive predictor) model &lt;- lm(sat_life ~ neg_life_experiences + relationship_status + neg_life_experiences*relationship_status, data = mydata) summary(model) How do you break down and understand a significant interactive predictor? Remember comparing correlations - go back to that! # Go back to the code we used to statistically compare correlations, and adapt it (if/where needed) to map onto NLE * relationship status predicting SWL... # First, we need to tell R which subgroups within our dataset we want to look at - so identify 0 and 1 from the &quot;relationship_status&quot; variable, and name each one. single &lt;- mydata[mydata$relationship_status == &quot;0&quot;, ] relationship &lt;- mydata[mydata$relationship_status == &quot;1&quot;, ] # Now we run the two correlations - we need to tell R first which subgroup to use from the naming we just did, and which continuous variable to correlate. cor.test(single$sat_life, single$neg_life_experiences, method = &quot;pearson&quot;) cor.test(relationship$sat_life, relationship$neg_life_experiences, method = &quot;pearson&quot;) # To compare the correlations statistically, we need the N and the r for each group. The r value is the final value in the output we just created - the final line, under corr. # To get the N for each group, we ran the &quot;summary&quot; earlier, but you can do it again to save scrolling. summary(mydata) # Now we can statistically compare our r values. Make a note of which group you consider to be &quot;1&quot; and which is &quot;2&quot;. For this, it will be 1 is single and 2 is in a relationship. # First, make sure &quot;cocor&quot; is ticked in the &quot;Packages&quot; tab. # r1 is the first r-value, in this case -0.5218863 # r2 is the second r-value, in this case -0.1741089 # n1 is the first sample size, in this case 92 # n2 is the second sample size, in this case 108 # the code looks like this, so just replace the values as needed... cocor.indep.groups(r1, r2, n1, n2) cocor.indep.groups(-0.5218863, -0.1741089, 92, 108) # Final thing to do - graph these two correlations on the same plot to aid interpretation. plot_cc &lt;- ggplot(mydata, aes(x = neg_life_experiences, y = sat_life, colour = relationship_status)) + geom_point(aes(shape = relationship_status)) + geom_smooth(aes(linetype = relationship_status), method = &quot;lm&quot;, se = FALSE) + labs(title = &quot;Negative life experiences vs Satisfaction with life by Relationship status&quot;, x = &quot;Negative life experiences&quot;, y = &quot;Satisfaction with life&quot;) + theme_classic() + scale_color_manual(values = c(&quot;0&quot; = &quot;grey&quot;, &quot;1&quot; = &quot;black&quot;)) + scale_linetype_manual(values = c(&quot;0&quot; = &quot;solid&quot;, &quot;1&quot; = &quot;dashed&quot;)) + scale_shape_manual(values = c(&quot;0&quot; = 16, &quot;1&quot; = 3)) #Now show the graph in the &quot;Plots&quot; window... print(plot_cc) 2.1.11 ASSUMPTIONS OF MULTIPLE REGRESSION # Finally, time to look at assumptions! # First, the the regression - you report the regression after the assumptions, but R will need the &quot;model&quot; for the assumptions code. model &lt;- lm(sat_life ~ psych_wellbeing + physical_wellbeing + relationship_wellbeing + neg_life_experiences + occ_status + relationship_status + home_location, data = mydata) summary(model) 2.1.11.1 MULTICOLLINEARITY # Multicollinearity, looking at r values across all continuous predictor variables. mydata %&gt;% dplyr::select(psych_wellbeing, physical_wellbeing, relationship_wellbeing, neg_life_experiences) %&gt;% correlation(p_adjust = &quot;none&quot;) # Multicollinearity, calculate VIF. vif_values &lt;- vif(model) print(vif_values) # Multicollinearity, calculate tolerance. This is, essentially 1 - the R2 (so variance explained). tolerance_value &lt;- 1 - summary(model)$r.squared print(tolerance_value) 2.1.11.2 DISTRIBUTION OF RESIDUALS # Create the histogram of residuals ggplot(mydata, aes(x = model$residuals)) + geom_histogram(binwidth = 0.5, color = &quot;black&quot;, fill = &quot;white&quot;) + labs(title = &quot;Histogram of Residuals&quot;, x = &quot;Residuals&quot;, y = &quot;Frequency&quot;) + theme_minimal() 2.1.11.3 HOMOSCEDASTICITY # Create the scatterplot for homoscedasticity ggplot(mydata, aes(x = model$fitted.values, y = model$residuals)) + geom_point() + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;) + labs(title = &quot;Residuals vs Fitted Values&quot;, x = &quot;Fitted Values&quot;, y = &quot;Residuals&quot;) + theme_minimal() 2.1.11.4 EVALUATE THE NUMBER OF OUTLIERS # Identify outliers using standardized residuals standardized_residuals &lt;- rstandard(model) # Print standardized residuals print(standardized_residuals) # Determine the number of outliers (absolute value greater than 2) outliers &lt;- sum(abs(standardized_residuals) &gt; 2) print(outliers) # Calculate the percentage of outliers percentage_outliers &lt;- (outliers / nrow(mydata)) * 100 print(percentage_outliers) Lecture Data Script: here. Workshop Data Script: Slides: here. "],["session-3-t-tests-and-anovas.html", "3 Session 3: t-Tests and ANOVAs", " 3 Session 3: t-Tests and ANOVAs "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
